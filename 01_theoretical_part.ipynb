{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression in Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary setup for Google Colab & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/ml-kiwi-com/ml-prague.git\n",
    "! pip install -r ml-prague/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ml-prague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src import plots\n",
    "from src import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 5, 100)\n",
    "n_samples = 5\n",
    "utils.generate_prior_gps(x, n_samples, std=9, length_scale=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.linspace(0, 5, 10)\n",
    "x_grid = np.linspace(0, 5, 100)\n",
    "y_train_actual = np.sin((x_train - 2.5) ** 2)\n",
    "n_samples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.generate_posterior_gps(x_train,x_grid, y_train_actual, n_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Distribution & Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.generate_posterior_gps(x_train,x_grid, y_train_actual, n_samples, show_predictor=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Function\n",
    "\n",
    "Let's use a nice 1D function as an example:\n",
    "\n",
    "$$ f(x) = (x \\cdot (10 - x)) \\cdot sin(2x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim = [0, 10]\n",
    "x_nbr = 1_000\n",
    "X = np.linspace(start=xlim[0], stop=xlim[1], num=x_nbr).reshape(-1, 1)\n",
    "y = utils.obj_function(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick training data - small number of samples as we assume they come a from heavy simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 7\n",
    "X_train, _X_test, y_train, _y_test = train_test_split(\n",
    "    X, y, train_size=train_size, random_state=3\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in *sklearn*\n",
    "\n",
    "We will use the [sklearn.gaussian_process.GaussianProcessRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) class.\n",
    "\n",
    "You can also use the scikit-learn [user guide](https://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process) for future reference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Introduction\n",
    "\n",
    "There are multiple kernels available in the sklearn implementation, for now, we will use the following:\n",
    "- ConstantKernel\n",
    "- RBF (Radial basis function)\n",
    "\n",
    "Kernel interaction is already natively implemented in the package, so we can use notation like:\n",
    "- `kernel_1 * kernel_2` for kernel multiplication\n",
    "- `kernel_1 + kernel_2` for kernel addition\n",
    "- etc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting GPR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the GPR using *RBF* kernel\n",
    "- define kernel\n",
    "- initialize the GPR\n",
    "- fit kernel hyperparameters: variance & length of scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(.1, 1e2))\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gaussian_process.fit(X_train, y_train)\n",
    "gaussian_process.kernel_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fitted kernel, we can generate GPR prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kriging_mean, kriging_std = gaussian_process.predict(X, return_std=True)\n",
    "plots.show_basic_kriging_plot_1D(\n",
    "    X,\n",
    "    y,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    kriging_mean,\n",
    "    kriging_std,\n",
    "    \"Gaussian process regression on noise-free dataset\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our GPR look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_process.kernel_.get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quality Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Kriging mean\n",
    "\n",
    "Standard regression metrics can be utilized for Kriging mean.\n",
    "- MSE / RMSE\n",
    "- MAE\n",
    "- R-squared\n",
    "\n",
    "The choice of the metric will always up to you as it should reflect the use case for which you are building the model.\n",
    "\n",
    "For the rest of this workshop, we will be using RMSE as our main metric for the Kriging mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_basic = round(utils.rmse(y, kriging_std, xlim, x_nbr), 2)\n",
    "mae_basic = round(utils.mae(y, kriging_std, xlim, x_nbr), 2)\n",
    "print(f\"Observed metrics \\n \\t RMSE: {rmse_basic} \\n \\t MAE: {mae_basic}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Design\n",
    "\n",
    "What should be the next point that we should generate so that we improve our model most?\n",
    "\n",
    "It depends what *most* means for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Approach\n",
    "\n",
    "Improve the model in the area where it performs worst.\n",
    "\n",
    "First, let's recall where we're at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.show_basic_kriging_plot_1D(\n",
    "    X,\n",
    "    y,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    kriging_mean,\n",
    "    kriging_std,\n",
    "    f\"Sequential Kriging: local approach - Rank=0, RMSE={rmse_basic}\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate sequentially and at each iteration, we add the point that currently has the highest Kriging error.\n",
    "\n",
    "This added point is then included in our training set and GPR is fitted once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 6\n",
    "X_train_local = X_train.copy()\n",
    "y_train_local = y_train.copy()\n",
    "kriging_std_local = kriging_std.copy()\n",
    "gaussian_process_local = gaussian_process\n",
    "\n",
    "for iter in range(n_iter):\n",
    "\n",
    "    rng_max = kriging_std_local.argmax()\n",
    "    new_x = X[rng_max]\n",
    "    new_y = y[rng_max]\n",
    "    X_train_local = np.append(X_train_local, [new_x], axis=0)\n",
    "    y_train_local = np.append(y_train_local, new_y)\n",
    "\n",
    "    gaussian_process.fit(X_train_local, y_train_local)\n",
    "    kriging_mean_local, kriging_std_local = gaussian_process.predict(X, return_std=True)\n",
    "\n",
    "    rmse_local = round(utils.rmse(y, kriging_mean_local, xlim, x_nbr), 2)\n",
    "\n",
    "    plots.show_basic_kriging_improvement_1D(\n",
    "        X,\n",
    "        y,\n",
    "        X_train_local,\n",
    "        y_train_local,\n",
    "        kriging_mean_local,\n",
    "        kriging_std_local,\n",
    "        new_x,\n",
    "        new_y,\n",
    "        f\"Sequential Kriging: local approach - Rank={iter + 1}, RMSE={rmse_local}\",\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how far we've got by generating a few new simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final RMSE after {n_iter} iterations: {rmse_local}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Approach\n",
    "\n",
    "Choosing a point that will reduce the overall uncertainty most.\n",
    "\n",
    "First, let's recall where we begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.show_basic_kriging_plot_1D(\n",
    "    X,\n",
    "    y,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    kriging_mean,\n",
    "    kriging_std,\n",
    "    f\"Sequential Kriging: global approach - Rank=0, RMSE={rmse_basic}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_global = X_train.copy()\n",
    "y_train_global = y_train.copy()\n",
    "kriging_std_global = kriging_std.copy()\n",
    "gaussian_process_global = gaussian_process\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we are choosing from the following 100 new potential points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_try = np.linspace(start=0, stop=10, num=100).reshape(-1, 1)\n",
    "y_try = gaussian_process.predict(X_try, return_std=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute integrated error when each try point is added.\n",
    "\n",
    "Keep the same hyperparameters to fasten the model build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1_new, k2_new = (\n",
    "    gaussian_process.kernel_.k1.constant_value,\n",
    "    gaussian_process.kernel_.k2.length_scale,\n",
    ")\n",
    "new_kernel = ConstantKernel(k1_new, constant_value_bounds=\"fixed\") * RBF(\n",
    "    length_scale=k2_new, length_scale_bounds=\"fixed\"\n",
    ")\n",
    "gaussian_process_try = GaussianProcessRegressor(\n",
    "    kernel=new_kernel, n_restarts_optimizer=9, optimizer=None\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over each hypothetical new points and compute resulting integrated error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integ_err = []\n",
    "for i in range(len(X_try)):\n",
    "    X_train_try = np.append(X_train_global, [X_try[i]], axis=0)\n",
    "    y_train_try = np.append(y_train_global, y_try[i])\n",
    "    gaussian_process_try.fit(X_train_try, y_train_try)\n",
    "    _, kriging_std_try = gaussian_process_try.predict(X, return_std=True)\n",
    "    integ_err.append(kriging_std_try.sum())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate sequentially and at each iteration, we add the point that will most reduce the overall uncertainty.\n",
    "\n",
    "This added point is then included in our training set and GPR is fitted once again.\n",
    "\n",
    "Integrated error needs to be recalculated each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 6\n",
    "for iter in range(n_iter):\n",
    "    new_rng = np.array(integ_err).argmin()\n",
    "    new_x = X_try[new_rng]\n",
    "    new_y = utils.obj_function(new_x)\n",
    "    y_try[new_rng] = new_y\n",
    "    X_train_global = np.append(X_train_global, [new_x], axis=0)\n",
    "    y_train_global = np.append(y_train_global, new_y)\n",
    "\n",
    "    gaussian_process_global.fit(X_train_global, y_train_global)\n",
    "    kriging_mean_global, kriging_std_global = gaussian_process_global.predict(\n",
    "        X, return_std=True\n",
    "    )\n",
    "\n",
    "    rmse_global = round(utils.rmse(y, kriging_mean_global, xlim, x_nbr), 2)\n",
    "\n",
    "    plots.show_basic_kriging_improvement_1D(\n",
    "        X,\n",
    "        y,\n",
    "        X_train_global,\n",
    "        y_train_global,\n",
    "        kriging_mean_global,\n",
    "        kriging_std_global,\n",
    "        new_x,\n",
    "        new_y,\n",
    "        f\"Sequential Kriging: global approach - Rank={iter + 1}, RMSE={rmse_global}\",\n",
    "    )\n",
    "\n",
    "    k1_new, k2_new = (\n",
    "        gaussian_process_global.kernel_.k1.constant_value,\n",
    "        gaussian_process_global.kernel_.k2.length_scale,\n",
    "    )\n",
    "    new_kernel = ConstantKernel(k1_new, constant_value_bounds=\"fixed\") * RBF(\n",
    "        length_scale=k2_new, length_scale_bounds=\"fixed\"\n",
    "    )\n",
    "    gaussian_process_try = GaussianProcessRegressor(\n",
    "        kernel=new_kernel, n_restarts_optimizer=9, optimizer=None\n",
    "    )\n",
    "\n",
    "    integ_err = []\n",
    "    for i in range(len(X_try)):\n",
    "        X_train_try = np.append(X_train_global, [X_try[i]], axis=0)\n",
    "        y_train_try = np.append(y_train_global, y_try[i])\n",
    "        gaussian_process_try.fit(X_train_try, y_train_try)\n",
    "        _, kriging_std_try = gaussian_process_try.predict(X, return_std=True)\n",
    "        integ_err.append(kriging_std_try.sum())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how far we've got by generating a few new simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final RMSE after {n_iter} iterations: {rmse_global}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EGO\n",
    "\n",
    "We aim to find the global maximum of the objective funciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_max_x = X_train[y_train.argmax()]\n",
    "current_max = y_train.max()\n",
    "print(f\"current maximum is {round(current_max,2)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's recall where we're at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.show_basic_kriging_plot_1D(\n",
    "    X,\n",
    "    y,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    kriging_mean,\n",
    "    kriging_std,\n",
    "    f\"Sequential Kriging: Optimisation - Rank=0, current max: {round(current_max,2)}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_optim = X_train.copy()\n",
    "y_train_optim = y_train.copy()\n",
    "kriging_mean_optim = kriging_mean.copy()\n",
    "kriging_std_optim = kriging_std.copy()\n",
    "gaussian_process_optim = gaussian_process\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will iteratively propose a potential new maximum, generate that point from the heavy simulator and repeat the process until the global maximum is reached within reasonable doubt.\n",
    "\n",
    "The potential for improvement of each point is evaluated as the integral of its PDF above the current maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 7\n",
    "\n",
    "for iter in range(n_iter):\n",
    "\n",
    "    ei_x = utils.expected_improvement(kriging_mean_optim, kriging_std_optim, current_max)\n",
    "    rng_max_ei = ei_x.argmax()\n",
    "    new_x = X[rng_max_ei]\n",
    "    new_y = utils.obj_function(new_x)\n",
    "    X_train_optim = np.append(X_train_optim, [new_x], axis=0)\n",
    "    y_train_optim = np.append(y_train_optim, new_y)\n",
    "    current_max_x = X_train_optim[y_train_optim.argmax()]\n",
    "    current_max = y_train_optim.max()\n",
    "\n",
    "    gaussian_process_optim.fit(X_train_optim, y_train_optim)\n",
    "    kriging_mean_optim, kriging_std_optim = gaussian_process_optim.predict(X, return_std=True)\n",
    "\n",
    "    plots.show_basic_kriging_improvement_1D(\n",
    "        X,\n",
    "        y,\n",
    "        X_train_optim,\n",
    "        y_train_optim,\n",
    "        kriging_mean_optim,\n",
    "        kriging_std_optim,\n",
    "        new_x,\n",
    "        new_y,\n",
    "        f\"Sequential Kriging: Optimisation - Rank={iter + 1}, current max: {round(current_max,2)}\",\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no more points with a reasonable potential to exceed the current maximum.\n",
    "\n",
    "The located maximum is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y = {round(current_max, 2)}\")\n",
    "print(f\"x = {round(X[y==current_max][0][0], 2)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to the practical part\n",
    "\n",
    "https://colab.research.google.com/github/ml-kiwi-com/ml-prague/blob/main/02_practical_part.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
